{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c4b9e47-0581-44dd-9f4e-3d1d93df6f4a",
   "metadata": {},
   "source": [
    "# Data Wrangling with Spark\n",
    "Welcome to assignment-2-part-2 for the BDA course where you will use spark together with pandas to \n",
    "perfom common data processing tasks which are often part of a larger data science project. Please use the ```simulated_cdrs``` dataset.\n",
    "\n",
    "By the end of the asssignment, you will have accomplished the following:\n",
    "- how to setup a data science project \n",
    "- appreciate effect of your design choices on effiency when dealing with large datasets\n",
    "- appreciate that how you sample data will vary alot depending on the nature of the data\n",
    "- ability to manipulate spark dataframes, use spark user defined functions\n",
    "- ability to switch betweeen spark dataframes which are distributed and pandass dataframes (which arent) as needed\n",
    "\n",
    "\n",
    "For this assignment, please use the [pyspark API documentation](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html) to get details about specific functions when you use them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30fa375-686f-485c-8833-2d5253eb3c28",
   "metadata": {},
   "source": [
    "## Python setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f07417f3-73a5-44f8-838c-31659d9934ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import other packages you need for this assignment\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2848b83b-8e0e-4e73-ba5c-aa638fc8ba49",
   "metadata": {},
   "source": [
    "## 1-Sampling data\n",
    "When working with large datasets, you often need to sample the data so that you can \n",
    "build and test your awesome algorithms on a smaller dataset without having to wait 10 minutes or hours depending on what you working with. However, sampling is not as straightfoward as you think, when the data is large and the sampling process complicated, you can even get stuck in the sampling stage. In this exercise, you will explore three strategies to achieve the same goal of sampling data. You will compare the efficiency of the three approaches.\n",
    "\n",
    "### 1-First sampling approach: isin()\n",
    "Given a list of users (their ```user_id```) that we have sampled from somewhere (in this case a pandas dataframe),\n",
    "we can use the function ```isin()``` on the spark dataframe (i.e the large input dataframe) and apply ```isin()``` \n",
    "to keep only the users in the list.\n",
    "### 2-Second sampling approach: join\n",
    "Once we have a list of user_id's whose data we need, we can convert that into a spark dataframe, \n",
    "then join with the input large dataframe. This will give us the required data, assuming we use thee correct join. However, joins can be very slow if you have a lot of data because its a shuffle operation.\n",
    "### 3-Third sampling approach: broadcast join\n",
    "When joining, if you have one smaller dataset and its small enough to fit into the memory of each worker, we can turn ShuffleHashJoin or SortMergeJoin into a BroadcastHashJoin. In broadcast join, the smaller dataFrame will be broadcasted to all worker nodes. Whats broadcasting?\n",
    "> Broadcast variables are read-only shared variables that are cached and available on all nodes in a cluster in-order to access or use by the tasks. Instead of sending this data along with every task, PySpark distributes broadcast variables to the workers using efficient broadcast algorithms to reduce communication costs.\n",
    "\n",
    "When joining, we can use the ```BROADCAST hint``` which  tells Spark to broadcast the smaller DataFrame when joining them with the bigger one. See how to use hints [here](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.hint.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "98d81e6d-2daf-4049-92d4-bd3ddbdd9415",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_users(sdf, min_events, num_users, method, output_csv):\n",
    "    \"\"\"\n",
    "    Sample users with minimum number of events\n",
    "    Arguments:\n",
    "    url -- spark dataframe tot work on\n",
    "    num_users -- number of users to sample\n",
    "    min_events -- minimum number of events\n",
    "    method -- eitheer join based meethod or isin() query\n",
    "    output_csv -- full path to ssave data for sampled users\n",
    "    \"\"\"\n",
    "    # =====================================\n",
    "    #       MARKING COMMENT \n",
    "    # A total of 13 points in this function\n",
    "    # =====================================\n",
    "    \n",
    "    # Use groupBy on appropriate column to get number of events\n",
    "    # per user (~ 1 line) and conveert the resulting dataframe to \n",
    "    # a pandas dataframe\n",
    "    pdf = None\n",
    "    \n",
    "    # Keep users with min_events as required\n",
    "    # Also, keep only num_users (~2 lines)\n",
    "    pdf = None\n",
    "    pdf2 = None\n",
    "    \n",
    "    # ===================================\n",
    "    # RETRIEVE DATA FOR THE SAMPLED USERS\n",
    "    # ===================================\n",
    "    # using the isin() method as follows:\n",
    "    # 1. Get a list of sampled users from pdf2 above\n",
    "    # 2. Use isin() method to sample data\n",
    "    # 3. Save data to CSV file.\n",
    "    # replace pass with your code (~3 lines)\n",
    "    if method == \"isin\":\n",
    "        pass\n",
    "    elif method == \"join-broadcast\":\n",
    "        # In order to use join to select the data, do the following:\n",
    "        # 1. dataconvert the pandas dataframe to spark\n",
    "        # 2. join the two dataframes making sure you select the correct join type\n",
    "        # this is broadcast join, so use hint as decsribed in docs\n",
    "        # 3. Save the data\n",
    "        # replace path with your code (~3-4 lines)\n",
    "        # If you have issues with conversion from pandas to spark, use a scheme\n",
    "        pass\n",
    "    else:\n",
    "        # this is regular join without broadcast\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9d3bf1-c7f0-4106-a1bd-6894cf090ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_running_times(input_csv_file, output_dir):\n",
    "    \"\"\"\n",
    "    Run the sample_users funcition above using three\n",
    "    different approaches and report the one with fasest\n",
    "    run time\n",
    "    Arguments:\n",
    "    input_csv_file -- full path to the input CSV file\n",
    "    output_dir --  directory (as a Path object) to save the sampled files\n",
    "    \"\"\"\n",
    "    \n",
    "    # create sparksession object and load the dataframe\n",
    "    spark = None\n",
    "    df = None\n",
    "    \n",
    "    methods = ['isin', \"join-broadcast\", 'join']\n",
    "    \n",
    "    # dictionary to hold time taken results\n",
    "    time_taken = None\n",
    "    \n",
    "    # Loop through the methods and do the following:\n",
    "    # 1. Create \n",
    "    for m in methods:\n",
    "        # Use joinpath() method on output_dir to create outcsv for this method\n",
    "        # the CSV file should have name: sample_method.csv\n",
    "        output_csv = None\n",
    "        # use datetime.now() method to records start time\n",
    "        start = None\n",
    "        # Call the sample_users() function (~1 line)\n",
    "        \n",
    "        # Record finish time \n",
    "        end = None\n",
    "        \n",
    "        # Calculate duration in minutes\n",
    "        # Use total_seconds() function on the time difference\n",
    "        duration = None\n",
    "        \n",
    "        # Add the duration to the results dictionary\n",
    "        # with method as key (~1 line)\n",
    "        \n",
    "    \n",
    "    # Get the best and worst running times\n",
    "    # Please use the dictionary to get the maximum and minimun\n",
    "    # values as required\n",
    "    best = None\n",
    "    worst = None\n",
    "    \n",
    "    print('Best approach is {} which took {} minutes'.format(best, int(time_taken[best])))\n",
    "    print('Worst approach is {} which took {} minutes'.format(worst, int(time_taken[worst])))\n",
    "    \n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc88329-b2d6-4c94-a9a1-7fae877078a3",
   "metadata": {},
   "source": [
    "## 2-Data exploration using spark\n",
    "In this part of the asssignment, you will get to perfom basic operations on a spark dataframe such \n",
    "as adding new columns, droping columns and more. Once you have summarized the data and its small enough, \n",
    "you can convert it to a pandas dataframe and do some analysis with pandas. Please complete the functiton below by \n",
    "following the steps below.\n",
    "\n",
    "### 1-Preprocess the data\n",
    "In this step, you will add new columns, drop columns and rename some columns. You will use the ```user defined function (udf)``` to add a date and datetime column to be used later in the analysis. This is because the original ```cdr_datetime``` is a sstring and is not time aware in Python. For this, please read on how to [define and use udf in spark](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.functions.udf.html#pyspark.sql.functions.udf)\n",
    "\n",
    "### 2-Get number of days\n",
    "One important thing we would like to know is how many days of data do we have. \n",
    "You will use the date column created above to get the number of days. During this process, \n",
    "note that you will be working with spark's ```Row``` data type.\n",
    "\n",
    "### 3-Get a distribution of calls by hour\n",
    "Also important to know is how many calls are made on each hour. To this end, \n",
    "you will use aggregate function to generate this and out the results as a heatmap for easier visualization.\n",
    "### 4-User attributes\n",
    "Finally, you will get one user attribute which is number of calls per day. Once you get this, you report the mean and median number of calls per day for users. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "250ef605-1da0-401e-a360-a65e3229ca4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def explore_data_with_spark(sdf=None, output_plot_file=None, output_heatmap=None):\n",
    "    \"\"\"\n",
    "    For quick examination of user activity, lets generate\n",
    "    user call count and do a simple plot.\n",
    "    \"\"\"\n",
    "    # ======================================\n",
    "    # DO QUICK PRE-PROCESSING\n",
    "    # ======================================\n",
    "    # 1.rename the following columns:\n",
    "    # cdr datetime =>cdr_datetime\", \"last calling cellid\"=>\"cell_id\")\n",
    "    # \"call duration\"=>\"call_duration\"\n",
    "    df2 = None\n",
    "    \n",
    "    # drop cdr type column\n",
    "    df3 = None\n",
    "    \n",
    "    # 2. add datetime and date by converting from string cdr_datetime\n",
    "    # Use Spark UDF to add date and datetime\n",
    "    date_format = '%Y%m%d%H%M%S'\n",
    "    \n",
    "    # define udf to convert from string to Python datetime\n",
    "    # To convert from string to datetime, use function  datetime.strptime()\n",
    "    # Use spark TimestampType() data type as the output\n",
    "    add_datetime = None\n",
    "    \n",
    "    # For date, same instructions ass above but use spark DateType()\n",
    "    add_date = None\n",
    "\n",
    "    # Now add new columns for datetime and 'date' by calling \n",
    "    # the udfs above\n",
    "    df4 = None\n",
    "    df5 = None\n",
    "    \n",
    "    # ======================================\n",
    "    # NUMBER OF DAYS IN THE DATA\n",
    "    # ======================================\n",
    "    # Get how many days are in the data by doing the following\n",
    "    # 1. Get unique dates by using distinct() on the spark dataframe\n",
    "    # 2. You can either use collect() on the result or covert to pandas dataframe\n",
    "    # 3. Use sorting to get the first (earliest) and last date \n",
    "    # These instructions assume you are using collect() which wil give us\n",
    "    # spark Row objects. Otherwise, if you end up using pandas, please \n",
    "    # write your \n",
    "    dates_rows = None\n",
    "    # use sorted() and list comprehension to sort the list of dates\n",
    "    sorted_dates = None\n",
    "    diff = None\n",
    "    # You can use days() function on the diff object\n",
    "    num_days = None\n",
    "\n",
    "    # ======================================\n",
    "    # AGGREGATE TO GET CALL COUNT BY HOUR\n",
    "    # ======================================\n",
    "    # define  udf to add hour column using attribute hour\n",
    "    # on a datetime\n",
    "    add_hr = None\n",
    "    \n",
    "    # define udf to add weekday as a number \n",
    "    # using python function weekday() on a date object\n",
    "    add_wkday = None\n",
    "    \n",
    "    # create dict with nums (starting from 0) as keys and \n",
    "    # days as values\n",
    "    day_dict = None\n",
    "    \n",
    "    # add hour column to the spark dataframe\n",
    "    dfHr = None\n",
    "    \n",
    "    # add wkday column\n",
    "    dfHr2 = None\n",
    "    \n",
    "    # group by to get week day and hour to get\n",
    "    # total events by day and hour and convert result\n",
    "    # to pandas dataframe\n",
    "    dfWkDay = None\n",
    "    \n",
    "    # use pandas map() function on column wkday\n",
    "    # to get the wkday name and add that as a column\n",
    "    dfWkDay['wkday_name'] = None\n",
    "    \n",
    "    dfWkDay.drop(labels=['wkday'], axis=1, inplace=True)\n",
    "    dfWkDayPivot = dfWkDay.pivot(index='wkday_name', columns='hr', values='count')\n",
    "    \n",
    "    # Create a heatmap with sns.heatmap() and the dfWkDayPivot dataframe\n",
    "    ax = None\n",
    "    \n",
    "    # Save output to file (~ 1 line)\n",
    "    \n",
    "    \n",
    "    # ==============================================\n",
    "    # GET USER ATTRIBUTES: CALLS PER DAY\n",
    "    # ============================================\n",
    "    # group user and count number of events\n",
    "    # convert resulting spark dataframe to pandas\n",
    "    dfGroup = None\n",
    "\n",
    "    # create a distribution plot of user call count using\n",
    "    # seaborn\n",
    "    ax = None\n",
    "\n",
    "    # save plot as png file(~ 1 line)\n",
    "\n",
    "    # report average number calls per day for each user\n",
    "    # by grouping by user_id and date\n",
    "    # and convert resullt to pandas dataframe\n",
    "    dfGroupDay = None\n",
    "    \n",
    "    \n",
    "    # get mean and median days based on\n",
    "    # grouped dataframe above\n",
    "    mean = None\n",
    "    median = None\n",
    "\n",
    "    # return mean, median, num_days"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d59b26-fdaf-4c26-9eb5-d438a5972b00",
   "metadata": {},
   "source": [
    "## 3-Put it all together.\n",
    "From the ```sample_users()``` function, you should have sample data for subset of users \n",
    "which you can use with the ```explore_data_with_spark()``` function. Note that you can make the number of users very small at the beginning ass you test the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "35f9beef-3e88-4e92-a7b9-708e1c9092c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# SAMPLING USER DATA\n",
    "# ==============================\n",
    "# setup input file and output directory\n",
    "input_cdrs = None\n",
    "\n",
    "# Convert the output dir str into a Path object\n",
    "# using Path() function\n",
    "outdir = None\n",
    "\n",
    "# Call the compare_running_times() function (~ 1 line)\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# DATA EXPLORATION\n",
    "# ==============================\n",
    "# create spark session\n",
    "spark = None\n",
    "\n",
    "# please use the sample users from previous function to reduce computation time\n",
    "# you can start experimenting witth very few user data\n",
    "# load a spark dataframe\n",
    "df = None\n",
    "\n",
    "# provide full paths to save distplot and heatmap\n",
    "out_heatmap = None\n",
    "out_distplot = None\n",
    "\n",
    "# Call the explore_data_with_spark() function\n",
    "mean, median, num_days = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63df3d9-a6c2-4b94-87d0-0014c254ca9c",
   "metadata": {},
   "source": [
    "### Question based on ```sample_users() function```\n",
    "1. Why are we sampling the data the way we are doing. Why can't we just randomly sample \n",
    "from the input dataframe ```sdf``` without worrying about the user id's and grouping users?\n",
    "2. What other spark function(s) (on a dataframe) could we have explored to achieve the same goal?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3426ea32-ace5-4ec9-b056-4940848f5fed",
   "metadata": {},
   "source": [
    "## Congratulations on completing this part of assignment!\n",
    "Here some notes on how the assingment will be marked. Full marks will be \n",
    "achieved based on two main criteria:\n",
    "- Code is able to run and saves all the outputs as required\n",
    "- The saved outputs are as expected: heatmap and distribution plot\n",
    "- The returned values for ```mean, median, num_days``` are accurate\n",
    "\n",
    "Also, please answer the question above in the same below the question. It will be marked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83ceeec-7527-453a-b4d0-abb6fd4a4eb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
